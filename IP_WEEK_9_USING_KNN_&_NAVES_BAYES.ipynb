{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IP WEEK 9 : USING KNN & NAVES BAYES",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JOSEPHINEGEND/IP-WEEK-9-KNN-NAIVES-BAYES/blob/master/IP_WEEK_9_USING_KNN_%26_NAVES_BAYES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzFbFr4MHTEs",
        "colab_type": "text"
      },
      "source": [
        "# Two Python Projects:\n",
        "# 1. Predicting Survival in the Titanic Disaster.\n",
        "# 2. Predicting whether an email is spam or ham."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Koggf9tGHTBy",
        "colab_type": "text"
      },
      "source": [
        "# 1.1 Introduction\n",
        "\n",
        "- RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early hours of 15 April 1912, after colliding with an iceberg during her maiden voyage from Southampton to New York City. \n",
        "- There were an estimated 2,224 passengers and crew aboard, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history.\n",
        "- RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line.\n",
        "-The Titanic was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, her architect, died in the disaster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d9SV4sWXslR",
        "colab_type": "text"
      },
      "source": [
        "*********************************************************\n",
        "\n",
        "The second dataset:\n",
        "* The \"spam\" concept is diverse and can be applied in advertisements for products/web sites, make money fast schemes, chain letters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uASShrCXHS_H",
        "colab_type": "text"
      },
      "source": [
        "## 1.1.1 Defining the Questions\n",
        "\n",
        "1. Predict if a passenger survived the sinking of the Titanic or not.\n",
        "2. Predict whether an email is spam or ham.\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh1CWljDKj_B",
        "colab_type": "text"
      },
      "source": [
        "## 1.1.2 Metrics for success\n",
        "* Since both projects are classification problems, we will use:\n",
        "     * Accuracy; threshold 85%\n",
        "* For target class imbalance we will use \n",
        "* (harmonic mean between the positive rate (precision) and the negative rate (Recall))\n",
        "     * F1 score; threhold 85%\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7F0Hk4EKj8X",
        "colab_type": "text"
      },
      "source": [
        "## 1.1.3 The Context\n",
        "\n",
        "* This is a challenge on Kaggle.com “Titanic: Machine Learning from Disaster” Competition.\n",
        "\n",
        "* In this challenge, we are asked to predict whether a passenger on the titanic would have been survived or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJXop2BXX4TP",
        "colab_type": "text"
      },
      "source": [
        "****************************************************\n",
        "* In the second project we aim to construct a personalized or a general purpose spam filter\n",
        "* Predicting whether an email is spam or ham from the given dataset.\n",
        "* For more info: https://archive.ics.uci.edu/ml/datasets/Spambase.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsla0RsxMQwT",
        "colab_type": "text"
      },
      "source": [
        "## 1.1.4 Experimental Design taken\n",
        " \n",
        " **Project 1: Predicting survival in the titanic Disaster**\n",
        " * Loading the dataset\n",
        " * Exploratory Data Analysis\n",
        " * Visualization\n",
        " * Data Cleaning\n",
        " * Features Engineering\n",
        " * Modeling: K-Nearest Neighbors Classifier (KNN)\n",
        " * Hyperparameter Tuning\n",
        " * Optimization techinques for KNN \n",
        " * Recommendations\n",
        " * Challenging the model: Random Forest Classifier\n",
        " * Conclusion\n",
        " \n",
        "**Project 2: Predicting whether an email is a spam or ham**\n",
        " * Loading the dataset\n",
        " * Exploratory Data Analysis\n",
        " * Visualization\n",
        " * Modelling: Naive Bayes Classifier: GaussianNB\n",
        " * Optimizing techniques for Gaussian Naive Bayes Classifier\n",
        " * Recommendations\n",
        " * Challenging the solution: Support Vector Classifier\n",
        " * Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc-HdK50Os4i",
        "colab_type": "text"
      },
      "source": [
        "## 1.1.5 Appropriateness of the data\n",
        "**Dataset 1 links:**\n",
        "* Train set: [link text](https://www.kaggle.com/c/titanic/download/train.csv)\n",
        "* Test set:[link text](https://www.kaggle.com/c/titanic/download/test.csv)\n",
        "\n",
        "The dataset contains the following fields:\n",
        "* Pclass Ticket class (: 1=upper, 2=middle, 3=lower)\n",
        "* Sex : Gender\n",
        "* Age : Age in years (fractional for babies)\n",
        "* Sibsp : Number of siblings and spouse. Sibling = brother, sister, stepbrother, stepsister.    Spouse = husband, wife (mistresses and fiancés were ignored)\n",
        "* Parch: Number of parents or children aboard the ship. Parent = mother, father. Child = daughter, son, stepdaughter, stepson. Some children travelled only with a nanny, therefore parch=0 for them.\n",
        "* Ticket: Ticket number (a string of characters)\n",
        "* Fare: Passenger fare (dollars)\n",
        "* Cabin: Cabin number (a string of characters)\n",
        "* Embarked: Port of embarkation (S=Southampton, Q=Queenstown (now Cobh), C=Cherbourg)\n",
        "\n",
        "\n",
        "**Dataset 2 link:**\n",
        "* [link text](https://archive.ics.uci.edu/ml/datasets/Spambase)\n",
        "* The collection of spam e-mails came from the postmaster and individuals who had filed spam.\n",
        "* The collection of non-spam e-mails came from filed work and personal e-mails.\n",
        "* Drawback :This dataset has no column names.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9z9S-NCHDxw",
        "colab_type": "text"
      },
      "source": [
        "# 1.2 Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfAdzLx4MAxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries we will use for this project analysis\n",
        "\n",
        "import pandas as pd  # for data processing\n",
        "import numpy as np  # for numerical calculations\n",
        "\n",
        "\n",
        "# Data Visualization Libraries\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "%matplotlib inline\n",
        "\n",
        "# \n",
        "import requests\n",
        "from io import StringIO\n",
        "import pandas_profiling as pp\n",
        "\n",
        "# Sklearn libraries for data preparartion and performance measures\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, KFold, cross_val_predict\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, normalize, Normalizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Algorithms\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrGRA0JeMqzi",
        "colab_type": "text"
      },
      "source": [
        "# 1.3 Loading the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaOOq1EYCn48",
        "colab_type": "text"
      },
      "source": [
        "- Loading the dataset by first uploading the dataset to colab can sometimes be tiresome and quite repititive.\n",
        "* Therefore, I will use a function that gets the data from google drive to google colab everytime we want to use.\n",
        "* This is way faster and efficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgSkdJI_Me7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the FIRST dataset (TRAIN)\n",
        "\n",
        "def get_file(url):\n",
        "  url = 'https://drive.google.com/uc?export=download&id=' + url.split('/')[-2]\n",
        "  raw_csv = requests.get(url).text\n",
        "  csv = StringIO(raw_csv)\n",
        "  return csv\n",
        "\n",
        "train = pd.read_csv(get_file('https://drive.google.com/file/d/1nQaDe_B3Hw136ot5B_1vaqfSg1oH7HNT/view?usp=sharing'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5uR8j7RMe46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the FIRST dataset (TEST)\n",
        "\n",
        "def get_file(url):\n",
        "  url = 'https://drive.google.com/uc?export=download&id=' + url.split('/')[-2]\n",
        "  raw_csv = requests.get(url).text\n",
        "  csv = StringIO(raw_csv)\n",
        "  return csv\n",
        "\n",
        "test = pd.read_csv(get_file('https://drive.google.com/file/d/1yPfpYVT7kwfzxgGVVeGEEwW3Q1pALNC2/view?usp=sharing'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV1mwCqZMe17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the SECOND dataset \n",
        "\n",
        "def get_file(url):\n",
        "  url = 'https://drive.google.com/uc?export=download&id=' + url.split('/')[-2]\n",
        "  raw_csv = requests.get(url).text\n",
        "  csv = StringIO(raw_csv)\n",
        "  return csv\n",
        "\n",
        "df = pd.read_csv(get_file('https://drive.google.com/file/d/1sT0dcWfyAguCpBpCphEWzP-UGmZJSxgB/view?usp=sharing'))\n",
        "                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jswA4aqJIvDd",
        "colab_type": "code",
        "outputId": "13976667-db25-4213-ecde-66f0da701ea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "# Creating column names for the spam base dataset 2\n",
        "# Since it has no columns names\n",
        "# Using random numbers\n",
        "\n",
        "values = np.random.random_integers(1, 60, 58)\n",
        "\n",
        "columns = {}\n",
        "column_base_name = 'Column'\n",
        "\n",
        "for i, value in enumerate(values):\n",
        "    columns['{:s}{:d}'.format(column_base_name, i)] = value\n",
        "\n",
        "df.columns = columns\n",
        "print(df.columns)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Column0', 'Column1', 'Column2', 'Column3', 'Column4', 'Column5',\n",
            "       'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11',\n",
            "       'Column12', 'Column13', 'Column14', 'Column15', 'Column16', 'Column17',\n",
            "       'Column18', 'Column19', 'Column20', 'Column21', 'Column22', 'Column23',\n",
            "       'Column24', 'Column25', 'Column26', 'Column27', 'Column28', 'Column29',\n",
            "       'Column30', 'Column31', 'Column32', 'Column33', 'Column34', 'Column35',\n",
            "       'Column36', 'Column37', 'Column38', 'Column39', 'Column40', 'Column41',\n",
            "       'Column42', 'Column43', 'Column44', 'Column45', 'Column46', 'Column47',\n",
            "       'Column48', 'Column49', 'Column50', 'Column51', 'Column52', 'Column53',\n",
            "       'Column54', 'Column55', 'Column56', 'Column57'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: This function is deprecated. Please call randint(1, 60 + 1) instead\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-YJPnlGYCD_",
        "colab_type": "text"
      },
      "source": [
        "# 1.4 EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B5cB103Me0x",
        "colab_type": "code",
        "outputId": "f4e447b6-983b-468d-ba47-67c385d7399c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# Viewing the first  five observations of the train dataset\n",
        "train.head(5)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "2            3         1       3  ...   7.9250   NaN         S\n",
              "3            4         1       1  ...  53.1000  C123         S\n",
              "4            5         0       3  ...   8.0500   NaN         S\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wO5-9jUMext",
        "colab_type": "code",
        "outputId": "2e6941bf-6a8b-47f5-f8ce-e7634e2694fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# Viewing the first five observations of the test dataset\n",
        "test.head(5)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>3</td>\n",
              "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>363272</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>2</td>\n",
              "      <td>Myles, Mr. Thomas Francis</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>240276</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>3</td>\n",
              "      <td>Wirz, Mr. Albert</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315154</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>896</td>\n",
              "      <td>3</td>\n",
              "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
              "      <td>female</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3101298</td>\n",
              "      <td>12.2875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass  ... Cabin Embarked\n",
              "0          892       3  ...   NaN        Q\n",
              "1          893       3  ...   NaN        S\n",
              "2          894       2  ...   NaN        Q\n",
              "3          895       3  ...   NaN        S\n",
              "4          896       3  ...   NaN        S\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O0-M7fEMevq",
        "colab_type": "code",
        "outputId": "522976da-9e5f-488a-a5c0-50175c944a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "# # Viewing the first five observations of the second dataset\n",
        "df.head(5)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column0</th>\n",
              "      <th>Column1</th>\n",
              "      <th>Column2</th>\n",
              "      <th>Column3</th>\n",
              "      <th>Column4</th>\n",
              "      <th>Column5</th>\n",
              "      <th>Column6</th>\n",
              "      <th>Column7</th>\n",
              "      <th>Column8</th>\n",
              "      <th>Column9</th>\n",
              "      <th>Column10</th>\n",
              "      <th>Column11</th>\n",
              "      <th>Column12</th>\n",
              "      <th>Column13</th>\n",
              "      <th>Column14</th>\n",
              "      <th>Column15</th>\n",
              "      <th>Column16</th>\n",
              "      <th>Column17</th>\n",
              "      <th>Column18</th>\n",
              "      <th>Column19</th>\n",
              "      <th>Column20</th>\n",
              "      <th>Column21</th>\n",
              "      <th>Column22</th>\n",
              "      <th>Column23</th>\n",
              "      <th>Column24</th>\n",
              "      <th>Column25</th>\n",
              "      <th>Column26</th>\n",
              "      <th>Column27</th>\n",
              "      <th>Column28</th>\n",
              "      <th>Column29</th>\n",
              "      <th>Column30</th>\n",
              "      <th>Column31</th>\n",
              "      <th>Column32</th>\n",
              "      <th>Column33</th>\n",
              "      <th>Column34</th>\n",
              "      <th>Column35</th>\n",
              "      <th>Column36</th>\n",
              "      <th>Column37</th>\n",
              "      <th>Column38</th>\n",
              "      <th>Column39</th>\n",
              "      <th>Column40</th>\n",
              "      <th>Column41</th>\n",
              "      <th>Column42</th>\n",
              "      <th>Column43</th>\n",
              "      <th>Column44</th>\n",
              "      <th>Column45</th>\n",
              "      <th>Column46</th>\n",
              "      <th>Column47</th>\n",
              "      <th>Column48</th>\n",
              "      <th>Column49</th>\n",
              "      <th>Column50</th>\n",
              "      <th>Column51</th>\n",
              "      <th>Column52</th>\n",
              "      <th>Column53</th>\n",
              "      <th>Column54</th>\n",
              "      <th>Column55</th>\n",
              "      <th>Column56</th>\n",
              "      <th>Column57</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>15</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Column0  Column1  Column2  Column3  ...  Column54  Column55  Column56  Column57\n",
              "0     0.21     0.28     0.50      0.0  ...     5.114       101      1028         1\n",
              "1     0.06     0.00     0.71      0.0  ...     9.821       485      2259         1\n",
              "2     0.00     0.00     0.00      0.0  ...     3.537        40       191         1\n",
              "3     0.00     0.00     0.00      0.0  ...     3.537        40       191         1\n",
              "4     0.00     0.00     0.00      0.0  ...     3.000        15        54         1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M6hXIC0Y4rh",
        "colab_type": "text"
      },
      "source": [
        "We will work on the first dataset with train and test sets then later work on the second dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0QRSW2YZDAh",
        "colab_type": "text"
      },
      "source": [
        "# PART 1 : DATASET 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mspduN1EdUR",
        "colab_type": "code",
        "outputId": "d3426a66-af6e-4a26-e744-c09521ab9237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# We will the last five observations of the train sets\n",
        "# we had earlier seen the head\n",
        "train.tail()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>887</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>Montvila, Rev. Juozas</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211536</td>\n",
              "      <td>13.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>888</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Graham, Miss. Margaret Edith</td>\n",
              "      <td>female</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>112053</td>\n",
              "      <td>30.00</td>\n",
              "      <td>B42</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>889</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>W./C. 6607</td>\n",
              "      <td>23.45</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>890</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Behr, Mr. Karl Howell</td>\n",
              "      <td>male</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>111369</td>\n",
              "      <td>30.00</td>\n",
              "      <td>C148</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>891</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Dooley, Mr. Patrick</td>\n",
              "      <td>male</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>370376</td>\n",
              "      <td>7.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     PassengerId  Survived  Pclass  ...   Fare Cabin  Embarked\n",
              "886          887         0       2  ...  13.00   NaN         S\n",
              "887          888         1       1  ...  30.00   B42         S\n",
              "888          889         0       3  ...  23.45   NaN         S\n",
              "889          890         1       1  ...  30.00  C148         C\n",
              "890          891         0       3  ...   7.75   NaN         Q\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MwtmgV1Z4Hp",
        "colab_type": "text"
      },
      "source": [
        "From the table above: \n",
        "\n",
        "We need to convert a lot of features into numeric ones later on, so that the machine learning algorithms can process them.\n",
        "\n",
        "Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. \n",
        "\n",
        "We can also spot some more features, that contain missing values (NaN = not a number), that we need to deal with.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahm2C3XdRagH",
        "colab_type": "code",
        "outputId": "f19d7b2a-031b-4976-e6ba-e5de88ec444d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# viewing summary information for our train set\n",
        "\n",
        "train.info()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            "PassengerId    891 non-null int64\n",
            "Survived       891 non-null int64\n",
            "Pclass         891 non-null int64\n",
            "Name           891 non-null object\n",
            "Sex            891 non-null object\n",
            "Age            714 non-null float64\n",
            "SibSp          891 non-null int64\n",
            "Parch          891 non-null int64\n",
            "Ticket         891 non-null object\n",
            "Fare           891 non-null float64\n",
            "Cabin          204 non-null object\n",
            "Embarked       889 non-null object\n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1bWv095Snh5",
        "colab_type": "text"
      },
      "source": [
        "Cabin column has a lot of missing data in both sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwRyIxW8EdN8",
        "colab_type": "code",
        "outputId": "b7eb23e9-7b74-463e-f70c-b1bf9ca8c847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "# checking for unique values \n",
        "\n",
        "cols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
        "\n",
        "for col in cols:\n",
        "  print(col)\n",
        "  print(train[col].unique())\n",
        "  print('\\n')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pclass\n",
            "[3 1 2]\n",
            "\n",
            "\n",
            "Sex\n",
            "['male' 'female']\n",
            "\n",
            "\n",
            "SibSp\n",
            "[1 0 3 4 2 5 8]\n",
            "\n",
            "\n",
            "Parch\n",
            "[0 1 2 5 3 4 6]\n",
            "\n",
            "\n",
            "Embarked\n",
            "['S' 'C' 'Q' nan]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69mQhum1EdJ_",
        "colab_type": "code",
        "outputId": "31c54daf-654f-4e8e-9c91-bb97036f01ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Summary statistics of the two sets\n",
        "# \n",
        "train.describe()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>714.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>446.000000</td>\n",
              "      <td>0.383838</td>\n",
              "      <td>2.308642</td>\n",
              "      <td>29.699118</td>\n",
              "      <td>0.523008</td>\n",
              "      <td>0.381594</td>\n",
              "      <td>32.204208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>257.353842</td>\n",
              "      <td>0.486592</td>\n",
              "      <td>0.836071</td>\n",
              "      <td>14.526497</td>\n",
              "      <td>1.102743</td>\n",
              "      <td>0.806057</td>\n",
              "      <td>49.693429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>223.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>20.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.910400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>446.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.454200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>668.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>891.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>512.329200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\n",
              "count   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\n",
              "mean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\n",
              "std     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\n",
              "min       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n",
              "25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n",
              "50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n",
              "75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\n",
              "max     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Af4yhYvVYN8",
        "colab_type": "text"
      },
      "source": [
        "Above we can see that 38% out of the training-set survived the Titanic.\n",
        "\n",
        "We can also see that the passenger ages range from 0.4 to 80.\n",
        "\n",
        "On top of that we can already detect some features, that contain missing values, like the ‘Age’ feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x81bSrTgEc_8",
        "colab_type": "code",
        "outputId": "258e0536-2a24-4e19-db70-48348232ccbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# checking the columns names\n",
        "\n",
        "train.columns"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
              "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1LEsUrILnx1",
        "colab_type": "text"
      },
      "source": [
        "# 1.4.1 VISUALIZATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSp45VaEcJLN",
        "colab_type": "text"
      },
      "source": [
        "Comparing Age and Sex and their contribution to survival of a person"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZoOL9Q0bjpQ",
        "colab_type": "code",
        "outputId": "5fdd7f03-8469-45fc-e58e-6f7bf7e23dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# Using Histograms\n",
        "# Plot using seaborn\n",
        "sns.set(style = 'whitegrid', context = 'notebook')\n",
        "# First we create variables labeling the survived column values 0 and 1\n",
        "survived = 'survived'\n",
        "not_survived = 'not survived'\n",
        "\n",
        "\n",
        "# creating a chart where our plots will appear\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
        "\n",
        "# creating women and male variables from the male and female variables in the dataset\n",
        "women = train[train['Sex']=='female']\n",
        "men = train[train['Sex']=='male']\n",
        "\n",
        "# Plotting the histogram of the women and specifying the bin sizes, and labels as we created earlier\n",
        "\n",
        "ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
        "ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
        "ax.legend()\n",
        "ax.set_title('Female')\n",
        "\n",
        "# Plotting the histogram of the men and specifying the bin sizes, and labels as we created earlier\n",
        "ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
        "ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\n",
        "ax.legend()\n",
        "_ = ax.set_title('Male')\n",
        "plt.show()\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEcCAYAAAD0haEFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLZK3El7eEZk",
        "colab_type": "text"
      },
      "source": [
        "* Survival chance of women are higher between 14 and 40\n",
        "* Men have a high probability of survival when they are between 18 and 30 years old.\n",
        "* In both generally infants have a little bit of higher chances of survival.\n",
        "* Certain ages hava increased odds of survival.\n",
        "* Creating age groups in our feature engineering may help to make every feature of the same scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg70p0d1f8-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NssaYZwwfv4-",
        "colab_type": "text"
      },
      "source": [
        "Embarked, Pclass, Sex and their importance on passanger survival."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6qyboppbjlr",
        "colab_type": "code",
        "outputId": "d5759bd8-cb0f-4516-ae0b-cb594bda2e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "# Using as Multi-plot grid for plotting conditional relationships.\n",
        "sns.set(style = 'whitegrid', context = 'notebook')\n",
        "FacetGrid = sns.FacetGrid(train, row = 'Embarked', size = 2.5, aspect = 1.6)\n",
        "FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex',\n",
        "             palette = None, order = None, hue_order = None)\n",
        "FacetGrid.add_legend()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/axisgrid.py:230: UserWarning: The `size` paramter has been renamed to `height`; please update your code.\n",
            "  warnings.warn(msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7f4c9e888ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAIQCAYAAABQad8uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 365.975x540 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6L-F4Clg6bM",
        "colab_type": "text"
      },
      "source": [
        "* Embarked seems to be correlated with survival, depending on the gender.\n",
        "* Women on port Q and on port S have a higher chance of survival. \n",
        "* Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGcVuLNrbjjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bIp2B-TiFYF",
        "colab_type": "text"
      },
      "source": [
        "Pclass importance on survival"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4LbY5qubjf6",
        "colab_type": "code",
        "outputId": "827eeebc-10d2-4514-f695-76f00ff9279f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "# Using a bar chart from seaborn\n",
        "sns.set(style = 'whitegrid', context = 'notebook')\n",
        "sns.barplot(x='Pclass', y='Survived', data=train)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4c9e55fcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAESCAYAAAAFYll6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHgAAY/8Pa9rmRItZPOkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zB3P9FUi0wu",
        "colab_type": "text"
      },
      "source": [
        "* It is clear that people from the First class had higher chances of survival.\n",
        "* Pclass is therefore an important feature to predict survival"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ii2OHxwEUUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L0_MOkUkCKa",
        "colab_type": "text"
      },
      "source": [
        "SibSp and Parch importance in survival\n",
        "\n",
        "SibSp and Parch would be better as a combined feature;\n",
        "\n",
        "that shows the total number of relatives, a person has on the Titanic.\n",
        "\n",
        "I will create it below and also a feature that shows if the passanger was alone or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-CT1EMYU0fN",
        "colab_type": "code",
        "outputId": "76278ad2-4268-4466-911f-7b0bf56001d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# We will create new features here in both the train and test sets -\n",
        "# this is part of feature engineering\n",
        "# using a for loop\n",
        "\n",
        "data = [train, test]\n",
        "\n",
        "for dataset in data:\n",
        "  dataset['relatives'] = dataset['SibSp'] + dataset['Parch'] # new column that has total relatives of board\n",
        "  dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0 # new column with two classes whether you are alone or not\n",
        "  dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n",
        "  dataset['not_alone'] = dataset ['not_alone'].astype(int) \n",
        "  \n",
        "  \n",
        "train['not_alone'].value_counts()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    537\n",
              "0    354\n",
              "Name: not_alone, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga1htqmqU0dK",
        "colab_type": "code",
        "outputId": "321e39b9-8f3f-4192-fbcb-fe4002ec17b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "# plotting the new features\n",
        "sns.set(style = 'whitegrid', context = 'notebook')\n",
        "axes = sns.factorplot('relatives','Survived', \n",
        "                      data=train, aspect = 2.5, )\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:3666: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFcCAYAAACazBxHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqESH8ocAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 900x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNnXOsRTqZNy",
        "colab_type": "text"
      },
      "source": [
        "There is a high probabilty of survival with 1 to 3 relatives\n",
        "\n",
        "A lower probailiti of surival exists if one had less than 1 or more than 3 \n",
        "\n",
        "(except for some cases with 6 relatives)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Bq60UvU0UX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4lLQZGtqyxR",
        "colab_type": "text"
      },
      "source": [
        "# 1.5 Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuaRa7c8syP6",
        "colab_type": "text"
      },
      "source": [
        "# 1.6 Feature Engineering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9nSXIAIs65t",
        "colab_type": "text"
      },
      "source": [
        "The two titles are together since I cant separate some actions that need feature engineering and those that need cleaning.\n",
        "\n",
        "For example; I need to create a new feature from an existing column then drop it.\n",
        "\n",
        "Whatever transformation is conducted on the train will also be done to the test set.\n",
        "\n",
        "Each step will include clear documentation to distinguish a cleaning or a feature engineering action.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4F2pq4Yq2DH",
        "colab_type": "text"
      },
      "source": [
        "## Missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMxC-jOrV1pT",
        "colab_type": "code",
        "outputId": "229a714d-f3fa-4151-da8d-ba74421aadd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# checking the mumber of missing values by column and sorting for the smallest\n",
        "\n",
        "Total = train.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "# Calculating percentages\n",
        "percent_1 = train.isnull().sum()/train.isnull().count()*100\n",
        "\n",
        "# rounding off to one decimal point\n",
        "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
        "\n",
        "# creating a dataframe to show the values\n",
        "missing_data = pd.concat([Total, percent_2], axis=1, keys=['Total', '%'])\n",
        "missing_data.head(5)\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total</th>\n",
              "      <th>%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Cabin</th>\n",
              "      <td>687</td>\n",
              "      <td>77.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>177</td>\n",
              "      <td>19.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Embarked</th>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>not_alone</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relatives</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Total     %\n",
              "Cabin        687  77.1\n",
              "Age          177  19.9\n",
              "Embarked       2   0.2\n",
              "not_alone      0   0.0\n",
              "relatives      0   0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSF0jsbeWDj7",
        "colab_type": "text"
      },
      "source": [
        "The Embarked feature has only 2 missing values, which can easily be filled by mode.\n",
        "\n",
        "It will be much more tricky, to deal with the ‘Age’ feature, which has 177 missing values.\n",
        "\n",
        "The ‘Cabin’ feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzha68QAtoDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gzqbz9LtkoP",
        "colab_type": "text"
      },
      "source": [
        "Step 1 : Cabin column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s10CSNvdV1mZ",
        "colab_type": "code",
        "outputId": "9c89ce02-29a2-48f7-fb74-83c7449b832c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "# checking the number of unique values in the cabin\n",
        "\n",
        "train.Cabin.unique()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan, 'C85', 'C123', 'E46', 'G6', 'C103', 'D56', 'A6',\n",
              "       'C23 C25 C27', 'B78', 'D33', 'B30', 'C52', 'B28', 'C83', 'F33',\n",
              "       'F G73', 'E31', 'A5', 'D10 D12', 'D26', 'C110', 'B58 B60', 'E101',\n",
              "       'F E69', 'D47', 'B86', 'F2', 'C2', 'E33', 'B19', 'A7', 'C49', 'F4',\n",
              "       'A32', 'B4', 'B80', 'A31', 'D36', 'D15', 'C93', 'C78', 'D35',\n",
              "       'C87', 'B77', 'E67', 'B94', 'C125', 'C99', 'C118', 'D7', 'A19',\n",
              "       'B49', 'D', 'C22 C26', 'C106', 'C65', 'E36', 'C54',\n",
              "       'B57 B59 B63 B66', 'C7', 'E34', 'C32', 'B18', 'C124', 'C91', 'E40',\n",
              "       'T', 'C128', 'D37', 'B35', 'E50', 'C82', 'B96 B98', 'E10', 'E44',\n",
              "       'A34', 'C104', 'C111', 'C92', 'E38', 'D21', 'E12', 'E63', 'A14',\n",
              "       'B37', 'C30', 'D20', 'B79', 'E25', 'D46', 'B73', 'C95', 'B38',\n",
              "       'B39', 'B22', 'C86', 'C70', 'A16', 'C101', 'C68', 'A10', 'E68',\n",
              "       'B41', 'A20', 'D19', 'D50', 'D9', 'A23', 'B50', 'A26', 'D48',\n",
              "       'E58', 'C126', 'B71', 'B51 B53 B55', 'D49', 'B5', 'B20', 'F G63',\n",
              "       'C62 C64', 'E24', 'C90', 'C45', 'E8', 'B101', 'D45', 'C46', 'D30',\n",
              "       'E121', 'D11', 'E77', 'F38', 'B3', 'D6', 'B82 B84', 'D17', 'A36',\n",
              "       'B102', 'B69', 'E49', 'C47', 'D28', 'E17', 'A24', 'C50', 'B42',\n",
              "       'C148'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbAcl2KPuCDb",
        "colab_type": "text"
      },
      "source": [
        "* A cabin number looks like ‘C85’ and the letter refers to the deck.\n",
        "* Therefore we’re going to extract these and create a new feature, that contains a persons deck. \n",
        "* Then we will convert the feature into a numeric variable. \n",
        "* The missing values will be converted to zero. \n",
        "* The actual decks of the titanic, ranged from A to G."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZtJsccDV1kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a new feature ; the deck column and their respective labels\n",
        "# We will use REGULAR EXPRESSION;\n",
        "# It's a string pattern written in a compact syntax, that allows us to quickly check\n",
        "# whether a given string matches or contains a given pattern. \n",
        "# we also use a for loop to iterate through both the test and train set.\n",
        "\n",
        "\n",
        "# importing regular expression\n",
        "import re\n",
        "\n",
        "\n",
        "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
        "data = [train, test]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n",
        "    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
        "    dataset['Deck'] = dataset['Deck'].map(deck)\n",
        "    dataset['Deck'] = dataset['Deck'].fillna(0)\n",
        "    dataset['Deck'] = dataset['Deck'].astype(int)\n",
        "    \n",
        "    \n",
        "# we can now drop the cabin feature in both sets\n",
        "train = train.drop(['Cabin'], axis=1)\n",
        "test = test.drop(['Cabin'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKUBfJWC2glM",
        "colab_type": "text"
      },
      "source": [
        "Step 2: Age column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM2xnh0L45Tr",
        "colab_type": "text"
      },
      "source": [
        "* For the age column with missing values,\n",
        "* We wil create an array coontaining random numbers\n",
        "* They will be computed based on; \n",
        "            * the mean age value,\n",
        "            * the spread of the age column (Standard deviation) and\n",
        "            * the number of missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2CEADZPv1VA",
        "colab_type": "code",
        "outputId": "242d558c-a5eb-49ff-8e4b-6a74f57cdea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# As before this will be effected both on the train and test set using a for loop\n",
        "\n",
        "data = [train, test]\n",
        "\n",
        "for dataset in data:\n",
        "    mean = train[\"Age\"].mean()\n",
        "    std = test[\"Age\"].std()\n",
        "    is_null = dataset[\"Age\"].isnull().sum() # number of null values\n",
        "    \n",
        "    \n",
        "    # computing random numbers between the mean, std and is_null\n",
        "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
        "    \n",
        "    # filling the NaN values in Age column with random values generated\n",
        "    age_slice = dataset[\"Age\"].copy()\n",
        "    age_slice[np.isnan(age_slice)] = rand_age    #Test element-wise for NaN and return result as a boolean array.\n",
        "    dataset[\"Age\"] = age_slice\n",
        "    dataset[\"Age\"] = train[\"Age\"].astype(int)\n",
        "    \n",
        "train[\"Age\"].isnull().sum()\n",
        "\n",
        "\n",
        "# the zero below shows we no longer have null values"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qirWNXg96fhU",
        "colab_type": "text"
      },
      "source": [
        "Step 3: Embarked column\n",
        "\n",
        "* We will fill with the most occurring value (mode)\n",
        "* There were only 2 missing values\n",
        "* I will then encode the categories into numerics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vb7kgG96Js2",
        "colab_type": "code",
        "outputId": "b49b3e06-2604-4ff3-ce69-892252e2d4aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# computing the mode\n",
        "\n",
        "train['Embarked'].describe()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     889\n",
              "unique      3\n",
              "top         S\n",
              "freq      644\n",
              "Name: Embarked, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWno20qo6Vhw",
        "colab_type": "code",
        "outputId": "a4ecb0d2-c751-4616-bc90-9f914c4a9aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# The mode is 'S'\n",
        "# filling with mode\n",
        "\n",
        "most_occur = 'S'\n",
        "data = [train, test]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].fillna(most_occur)\n",
        "    \n",
        "train[\"Embarked\"].isnull().sum()\n",
        "\n",
        "\n",
        "# we have no null value as the output is zero."
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyaHBMQfCgwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding the categories\n",
        "\n",
        "ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
        "data = [train, test]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].map(ports)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlcO9lub7okZ",
        "colab_type": "text"
      },
      "source": [
        "Step 4: Fare\n",
        "\n",
        "* Converting fare column data type from float to integer.\n",
        "* Integer operations are slightly faster than floats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxQVecgG7ksd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [train, test]\n",
        "\n",
        "for dataset in data:\n",
        "  dataset['Fare'] = dataset['Fare'].fillna(0)\n",
        "  dataset['Fare'] = dataset['Fare'].astype(int)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot-k8ogu-CDE",
        "colab_type": "text"
      },
      "source": [
        "Step 5: Name Column\n",
        "\n",
        "* Here we will extract Titles from the Name\n",
        "* We will create a feature out of that to test if Title was an important feature on survival\n",
        "* We will use regular expression as before to search for the title.\n",
        "* we will later drop the Name column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L3jnzUs9gbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will perform this on both sets using as for loop.\n",
        "\n",
        "data = [train, test]\n",
        "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "\n",
        "for dataset in data:\n",
        "    # extract titles\n",
        "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "    # replace titles with a more common title or as Rare\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
        "                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "    # convert titles into numbers\n",
        "    dataset['Title'] = dataset['Title'].map(titles)\n",
        "    # filling NaN with 0, to get safe\n",
        "    dataset['Title'] = dataset['Title'].fillna(0)\n",
        "train = train.drop(['Name'], axis=1)\n",
        "test = test.drop(['Name'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ5wCkZwAjU0",
        "colab_type": "text"
      },
      "source": [
        "Step 6 : Sex Column\n",
        "\n",
        "*  Converting them into numeric in both datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5r4Ge54_qIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gender = {\"male\":0, \"female\":1}\n",
        "\n",
        "data = [train, test]\n",
        "\n",
        "for dataset in data:\n",
        "  dataset['Sex'] = dataset['Sex'].map(gender)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBxortFyBQ4t",
        "colab_type": "text"
      },
      "source": [
        "Step 7: Ticket\n",
        "\n",
        "* This columns has a very high cardinality.\n",
        "* We will drop it for now in both datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFJBPwGQBOxS",
        "colab_type": "code",
        "outputId": "6c5ab485-ea54-411a-8517-eb72fd36b95b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# checking the unique values\n",
        "train.Ticket.describe()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count      891\n",
              "unique     681\n",
              "top       1601\n",
              "freq         7\n",
              "Name: Ticket, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LwLtRSNBpQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dropping Ticket column from both sets\n",
        "train = train.drop(['Ticket'], axis=1)\n",
        "test = test.drop(['Ticket'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tikuAVMCC_Vk",
        "colab_type": "text"
      },
      "source": [
        "## Creating Categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKdvRfIkCUNO",
        "colab_type": "text"
      },
      "source": [
        "Step 8: Age Categories\n",
        "\n",
        "* From the charts certain Ages to influenced survival.\n",
        "* We want the Ages to be on the same scale.\n",
        "\n",
        "* First we will convert it from float into integer. \n",
        "* Then we will create the new ‘AgeGroup” variable, \n",
        "* by categorizing every age into a group.\n",
        "* N/B: It is important to place attention on how we form these groups, since we do not want for example that 80% of our data  to fall into a particular group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZy4IP7uCN0Y",
        "colab_type": "code",
        "outputId": "af261a74-5ea0-4d78-b92b-755946d0fea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "data = [train, test]\n",
        "for dataset in data:\n",
        "    dataset['Age'] = dataset['Age'].astype(int)\n",
        "    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n",
        "    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n",
        "    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n",
        "    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n",
        "    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n",
        "    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n",
        "    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n",
        "    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n",
        "\n",
        "# let's see how it's distributed\n",
        "train['Age'].value_counts()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    164\n",
              "6    157\n",
              "5    147\n",
              "3    134\n",
              "2    126\n",
              "1     95\n",
              "0     68\n",
              "Name: Age, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCHTnXLzFwPp",
        "colab_type": "text"
      },
      "source": [
        "Step 9: Fare Categories\n",
        "\n",
        "* We will create the various categories like the Age,\n",
        "* But it isn’t that easy, \n",
        "* because if we cut the range of the fare values into a few equally big categories,\n",
        "* 80% of the values would fall into the first category. \n",
        "* This can be seen using a histogram of the Fare column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qs3mu-_GAZk",
        "colab_type": "code",
        "outputId": "7ba50809-650d-4bbf-9027-c8003f0e3a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "# Fare column histogram\n",
        "\n",
        "sns.distplot(train.Fare)\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4c9e4a3160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAESCAYAAADuVeJ5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEmk4sUvQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoW7o0GUFKdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating Fare categories in both sets of data\n",
        "\n",
        "data = [train, test]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
        "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
        "    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n",
        "    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n",
        "    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQBFt6LhI4SQ",
        "colab_type": "text"
      },
      "source": [
        "## Creating new Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1TSFq2eI69j",
        "colab_type": "text"
      },
      "source": [
        "Step 10: Age * PClass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9VkbW3VGHYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [train, test]\n",
        "for dataset in data:\n",
        "    dataset['Age_Class']= dataset['Age']* dataset['Pclass']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aad53qIFJSmz",
        "colab_type": "text"
      },
      "source": [
        "Step 11: Fare per Person"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "080kd4UtJSM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in data:\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pin3kFo-LNVl",
        "colab_type": "text"
      },
      "source": [
        "Step 12: Passenger ID\n",
        "\n",
        "* This columns does not contribute to a person's survival.\n",
        "* We will drop it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_1j89vzLcDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train.drop(['PassengerId'], axis=1)\n",
        "test = test.drop(['PassengerId'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8gkNrRtKAL4",
        "colab_type": "code",
        "outputId": "cc252f4e-e11f-4703-b0b8-86b30e2d5ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# Viewing our train set before modelling\n",
        "\n",
        "train.head()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>relatives</th>\n",
              "      <th>not_alone</th>\n",
              "      <th>Deck</th>\n",
              "      <th>Title</th>\n",
              "      <th>Age_Class</th>\n",
              "      <th>Fare_Per_Person</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Survived  Pclass  Sex  Age  ...  Deck  Title  Age_Class  Fare_Per_Person\n",
              "0         0       3    0    2  ...     8      1          6                0\n",
              "1         1       1    1    5  ...     3      3          5                1\n",
              "2         1       3    1    3  ...     8      2          9                0\n",
              "3         1       1    1    5  ...     3      3          5                1\n",
              "4         0       3    0    5  ...     8      1         15                1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MebHNIwK4Pt",
        "colab_type": "code",
        "outputId": "ed4cf732-ce47-45cf-b66c-1456cdbe5183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>relatives</th>\n",
              "      <th>not_alone</th>\n",
              "      <th>Deck</th>\n",
              "      <th>Title</th>\n",
              "      <th>Age_Class</th>\n",
              "      <th>Fare_Per_Person</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pclass  Sex  Age  SibSp  ...  Deck  Title  Age_Class  Fare_Per_Person\n",
              "0       3    0    2      0  ...     8      1          6                0\n",
              "1       3    1    5      1  ...     8      3         15                0\n",
              "2       2    0    3      0  ...     8      1          6                1\n",
              "3       3    0    5      0  ...     8      1         15                1\n",
              "4       3    1    5      1  ...     8      3         15                0\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEEpqPEAKWOA",
        "colab_type": "text"
      },
      "source": [
        "# 1.7 Building  Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmHG53PrOXds",
        "colab_type": "text"
      },
      "source": [
        "## 1.7.1 K-Nearest Neighbours Classifier\n",
        "* KNN\n",
        "* It is a supervised Machine learning algorithm\n",
        "* It's non-parametric as it use flexible number of parameters.\n",
        "* It makes few assumptions about the data.\n",
        "* Also, an instance based algorithm;\n",
        "* i.e. knn looks at the nearest neighbours to decide what any queried point should be.\n",
        "* It stores all the available cases and classifies new cases by a majority vote of its K neighbours. \n",
        "\n",
        "Advantages\n",
        "* Knn is easy to use.\n",
        "* Requires quick calculation time.\n",
        "* It does not make assumptions about the data.\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "* The accuracy of the algorithm depends on the quality of the data.\n",
        "* One needs to find an optimal k value (number of nearest neighbors).\n",
        "* It is poor at classifying data points in a boundary where they can be classified one way or another.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQY3RSxmTvLG",
        "colab_type": "text"
      },
      "source": [
        "* Creating a model using the train and test sets as given"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2n1ybFEKpCW",
        "colab_type": "code",
        "outputId": "c2eb07a4-c786-4fa5-d4ca-63f4f251aba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Splitting the data into test and train sets\n",
        "\n",
        "X_train = train.drop('Survived', axis = 1)\n",
        "y_train = train['Survived']\n",
        "X_test = test.copy()\n",
        "\n",
        "\n",
        "# Instantiating our model and setting k as 3\n",
        "knn = KNeighborsClassifier(n_neighbors=3) \n",
        "\n",
        "# Training our model\n",
        "knn.fit(X_train, y_train) \n",
        "\n",
        "# Making Predictions using our model \n",
        "\n",
        "y_pred = knn.predict(X_test) \n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score\n",
        "acc_knn = round(knn.score(X_train, y_train) * 100, 1)\n",
        "acc_knn"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85.3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egaD0g7Yu14w",
        "colab_type": "text"
      },
      "source": [
        "* The train set yields 87% accuracy without splitting it further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MYtlImOOuyYc",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmvAIGXSUT54",
        "colab_type": "text"
      },
      "source": [
        "Splitting the train set into 80, 20 sets as instructed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xikAR0dzKo8s",
        "colab_type": "code",
        "outputId": "a353239d-a107-4549-9a58-52652df67f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# Splitting the data into features and target variable\n",
        "\n",
        "X = train.drop('Survived', axis = 1)\n",
        "y = train.Survived\n",
        "\n",
        "\n",
        "# Splitting the data into test and train sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 10)\n",
        "\n",
        "\n",
        "# Instantiating our model\n",
        "# Using default  k= 5\n",
        "knc = KNeighborsClassifier()\n",
        "\n",
        "\n",
        "# Fitting the KNN classifier to our train data set\n",
        "model2 = knc.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Making prediction the created model\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8212290502793296\n",
            "\n",
            "\n",
            "[[103  14]\n",
            " [ 18  44]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       117\n",
            "           1       0.76      0.71      0.73        62\n",
            "\n",
            "    accuracy                           0.82       179\n",
            "   macro avg       0.80      0.80      0.80       179\n",
            "weighted avg       0.82      0.82      0.82       179\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuO59BRKdapb",
        "colab_type": "text"
      },
      "source": [
        "* The 80,20 split model yielded 80% accuracy.\n",
        "* Interpreting the confusion matrix;\n",
        "* The first row is about the not-survived-predictions:\n",
        "     * 96 passengers were correctly classified as not survived (called true negatives) \n",
        "     * 21 were wrongly classified as not survived (false positives).\n",
        "* The second row is about the survived-predictions: \n",
        "     * 17 passengers where wrongly classified as survived (false negatives) and\n",
        "     * 45 were correctly classified as survived (true positives)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXFvfPdbTsnj",
        "colab_type": "text"
      },
      "source": [
        "Splitting the train set into 70, 30 sets as instructed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo13dSSbKo6e",
        "colab_type": "code",
        "outputId": "cc59497f-ebc9-4049-88b1-4f8de81957aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# Splitting the data into features and target variable\n",
        "\n",
        "X = train.drop('Survived', axis = 1)\n",
        "y = train.Survived\n",
        "\n",
        "\n",
        "# Splitting the data into test and train sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 10)\n",
        "\n",
        "\n",
        "# Fitting the KNN classifier to our train data set\n",
        "model3 = knc.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Making prediction the created model\n",
        "y_pred = model3.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8171641791044776\n",
            "\n",
            "\n",
            "[[149  25]\n",
            " [ 24  70]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86       174\n",
            "           1       0.74      0.74      0.74        94\n",
            "\n",
            "    accuracy                           0.82       268\n",
            "   macro avg       0.80      0.80      0.80       268\n",
            "weighted avg       0.82      0.82      0.82       268\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjRgq5uvvdqn",
        "colab_type": "text"
      },
      "source": [
        "* The 70,30 split model yielded 75% accuracy.\n",
        "* This is lower compared to the 80,20 model.\n",
        "* Interpreting the confusion matrix.\n",
        "* The first row is about the not-survived-predictions:\n",
        "     * 131 passengers were correctly classified as not survived (called true negatives) \n",
        "     * 43 were wrongly classified as not survived (false positives).\n",
        "* The second row is about the survived-predictions: \n",
        "     * 24 passengers where wrongly classified as survived (false negatives) and\n",
        "     * 70 were correctly classified as survived (true positives)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMG-_ks8U7qQ",
        "colab_type": "text"
      },
      "source": [
        "Splitting the train set into 60, 40 sets as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJc7O8s_U-hi",
        "colab_type": "code",
        "outputId": "568eaf24-0704-4c0e-f4a2-4498bc4bc775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# Splitting the data into features and target variable\n",
        "\n",
        "X = train.drop('Survived', axis = 1)\n",
        "y = train.Survived\n",
        "\n",
        "\n",
        "# Splitting the data into test and train sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 10)\n",
        "\n",
        "# Instantiating our model\n",
        "knc = KNeighborsClassifier(n_neighbors = 3)\n",
        "\n",
        "# Fitting the KNN classifier to our train data set\n",
        "model3 = knc.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Making prediction the created model\n",
        "y_pred = model3.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7955182072829131\n",
            "\n",
            "\n",
            "[[192  37]\n",
            " [ 36  92]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       229\n",
            "           1       0.71      0.72      0.72       128\n",
            "\n",
            "    accuracy                           0.80       357\n",
            "   macro avg       0.78      0.78      0.78       357\n",
            "weighted avg       0.80      0.80      0.80       357\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBeaaMAOwOy2",
        "colab_type": "text"
      },
      "source": [
        "* The 60,40 split model yielded 76% accuracy.\n",
        "* This is higher compared to the 70,30 (75%); but lower than 80,20(80%).\n",
        "* Interpreting the confusion matrix.\n",
        "* The first row is about the not-survived-predictions:\n",
        "     * 179 passengers were correctly classified as not survived (called true negatives) \n",
        "     * 50 were wrongly classified as not survived (false positives).\n",
        "* The second row is about the survived-predictions: \n",
        "     * 35 passengers where wrongly classified as survived (false negatives) and\n",
        "     * 93 were correctly classified as survived (true positives)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU8MOJvyWrUQ",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter Tuning\n",
        "\n",
        "Parameters to tune in KNN\n",
        "\n",
        "* k's\n",
        "* Distance Metric\n",
        "* Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBulLhSfVal1",
        "colab_type": "text"
      },
      "source": [
        "### Optimization Techniques for KNN\n",
        "\n",
        "we can optimize KNN by:\n",
        "* Dimensionality Reduction with Linear Discriminant Analysis\n",
        "* Rescaling our data which makes the distance metric more meaningful. \n",
        "* Changing the distance **metric** for different applications.\n",
        "* Implementing weighted voting\n",
        "* Applying appropriate nearest-neighbor techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pse2jEDUVFNS",
        "colab_type": "code",
        "outputId": "8b6a8ede-377c-4ec5-9837-e99bcbc73df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "# Splitting the data into features and target variable\n",
        "\n",
        "X = train.drop('Survived', axis = 1)\n",
        "y = train.Survived\n",
        "\n",
        "\n",
        "# Splitting the data into test and train sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 10)\n",
        "\n",
        "# scaling our Data\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# Reducing the dimensions in our dataset\n",
        "# Specifying the number of components\n",
        "lda = LDA(n_components=7)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "\n",
        "# Searching the best parameters using Rnadom Search\n",
        "params = { 'n_neighbors' : range(1,10),\n",
        "          'metric' : ['euclidean', 'manhattan', 'minkowski'],\n",
        "          'weights': ['uniform', 'distance']}\n",
        "          \n",
        "kfold = KFold(n_splits = 5, random_state = 10)  # specifying number of folds for cross validation\n",
        "        \n",
        "rs = RandomizedSearchCV(KNeighborsClassifier(),\n",
        "                       params,  cv = kfold,\n",
        "                       n_jobs = -1)\n",
        "          \n",
        "rs = rs.fit(X_train, y_train)\n",
        "rs.best_params_\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(13, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
            "  warnings.warn(\"Variables are collinear.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metric': 'euclidean', 'n_neighbors': 7, 'weights': 'uniform'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZVGixsD0TUC",
        "colab_type": "text"
      },
      "source": [
        "Using the Best parameters to assess the best split KNN model (80,20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1rfCrFhaV49",
        "colab_type": "code",
        "outputId": "c07dfd00-9a5e-43a6-e95e-7bdcac53de12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Splitting the data into features and target variable\n",
        "\n",
        "X = train.drop('Survived', axis = 1)\n",
        "y = train.Survived\n",
        "\n",
        "\n",
        "# Splitting the data into test and train sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 10)\n",
        "\n",
        "\n",
        "# scaling our Data\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# Reducing the dimensions in our dataset\n",
        "# Specifying the number of components\n",
        "lda = LDA(n_components=7)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "# Instantiating our model\n",
        "# Using our best parameters using random search\n",
        "knc = KNeighborsClassifier(metric = 'euclidean', n_neighbors= 9, weights = 'uniform')\n",
        "\n",
        "# Fitting the KNN classifier to our train data set\n",
        "model2 = knc.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Making prediction the created model\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8100558659217877\n",
            "\n",
            "\n",
            "[[100  17]\n",
            " [ 17  45]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       117\n",
            "           1       0.73      0.73      0.73        62\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.79      0.79      0.79       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(13, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
            "  warnings.warn(\"Variables are collinear.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI0FyJ5q1lio",
        "colab_type": "text"
      },
      "source": [
        "* There is great improvement in our tuned model.\n",
        "* The wrongly classified classes actually reduced significantly.\n",
        "* Interprating the confusion matrix:\n",
        "* The first row is about the not-survived-predictions:\n",
        "     * 104 passengers were correctly classified as not survived (called true negatives) \n",
        "     * 13 were wrongly classified as not survived (false positives).\n",
        "* The second row is about the survived-predictions: \n",
        "     * 17 passengers where wrongly classified as survived (false negatives) and\n",
        "     * 45 were correctly classified as survived (true positives).\n",
        "     \n",
        "* The F1 score = 87%, Recall= 89% & Precision = 86% also improved greatly\n",
        "* Our model predicts 86% of the time, a passengers survival correctly (precision).\n",
        "* The recall tells us that it predicted the survival of 89% of the people who actually survived."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nctI0ax54406",
        "colab_type": "text"
      },
      "source": [
        "## 1.7.2 Recommendations\n",
        "\n",
        "**From the analysis: Tuning and Optimizing the KNN model is highly recommended to improve the performance of the model.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1fakek14Xi3",
        "colab_type": "text"
      },
      "source": [
        "## 1.7.3 Challenging the solution \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChnQe7Ke8BEG",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Classifier\n",
        "\n",
        "* Using Random Forest which is an Ensemble algorithm.\n",
        "* Also considering the Train data without splitting it further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0VLfdMW4s0J",
        "colab_type": "code",
        "outputId": "2ca63296-fb83-4461-b895-9b5787653c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Random Forest\n",
        "\n",
        "# Splitting the data into test and train sets as provided earlier\n",
        "# NOTE: No further split is done.\n",
        "\n",
        "X_train = train.drop('Survived', axis = 1)\n",
        "Y_train = train['Survived']\n",
        "X_test = test.copy()\n",
        "\n",
        "# Instantiating our model\n",
        "# Training the model\n",
        "random_forest = RandomForestClassifier(criterion = \"gini\", \n",
        "                                       min_samples_leaf = 1, \n",
        "                                       min_samples_split = 10,   \n",
        "                                       n_estimators=100, \n",
        "                                       max_features='auto', \n",
        "                                       oob_score=True, \n",
        "                                       random_state=10,\n",
        "                                       n_jobs=-1) #  Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
        "random_forest.fit(X_train, Y_train)\n",
        "\n",
        "# Making predictions\n",
        "Y_prediction = random_forest.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "random_forest.score(X_train, Y_train)\n",
        "\n",
        "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
        "print(round(acc_random_forest,2,), \"%\")\n",
        "\n",
        "# Generating cross-validated estimates for each input data point\n",
        "predictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\n",
        "\n",
        "\n",
        "print(confusion_matrix(Y_train, predictions))\n",
        "print(classification_report(Y_train, predictions))\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88.78 %\n",
            "[[494  55]\n",
            " [ 94 248]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       549\n",
            "           1       0.82      0.73      0.77       342\n",
            "\n",
            "    accuracy                           0.83       891\n",
            "   macro avg       0.83      0.81      0.82       891\n",
            "weighted avg       0.83      0.83      0.83       891\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJuA9poh8eW0",
        "colab_type": "text"
      },
      "source": [
        "* The Random Forest Classifier model yields 89% accuracy.\n",
        "\n",
        "* This is a better model compared to KNN.\n",
        "* However, tunning may be needed to assess the model adequately.\n",
        "\n",
        "* The Recall score increased compared to other models.\n",
        "\n",
        "* Interprating the confusion matrix:\n",
        "* The first row is about the not-survived-predictions:\n",
        "     * 492 passengers were correctly classified as not survived (called true negatives) \n",
        "     * 57 were wrongly classified as not survived (false positives).\n",
        "* The second row is about the survived-predictions: \n",
        "     * 94 passengers where wrongly classified as survived (false negatives) and\n",
        "     * 248 were correctly classified as survived (true positives).\n",
        "     \n",
        "* The F1 score = 87%, Recall= 90% & Precision = 84% also improved greatly\n",
        "* Our model predicts 86% of the time, a passengers survival correctly (precision).\n",
        "* The recall tells us that it predicted the survival of 89% of the people who actually survived."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K49nTMPk9mgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLC1qN-N1gAJ",
        "colab_type": "text"
      },
      "source": [
        "# 1.7.4 Conclusion\n",
        "* From the analysis: Tuning and Optimizing the KNN model is highly recommended to improve the performance of the model.**\n",
        "* Splitting the data into 80,20 train and test sets was the best in the Titanic dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4Yg6_jsCy8M",
        "colab_type": "text"
      },
      "source": [
        "# PART 2: DATASET 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNv384BpEAWK",
        "colab_type": "text"
      },
      "source": [
        "# 2.1 EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTmeIGMuC8kM",
        "colab_type": "code",
        "outputId": "4d24e825-f5f1-4856-82bb-b9bc93d7e0a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "# Checking the first five observations\n",
        "df.head()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column0</th>\n",
              "      <th>Column1</th>\n",
              "      <th>Column2</th>\n",
              "      <th>Column3</th>\n",
              "      <th>Column4</th>\n",
              "      <th>Column5</th>\n",
              "      <th>Column6</th>\n",
              "      <th>Column7</th>\n",
              "      <th>Column8</th>\n",
              "      <th>Column9</th>\n",
              "      <th>Column10</th>\n",
              "      <th>Column11</th>\n",
              "      <th>Column12</th>\n",
              "      <th>Column13</th>\n",
              "      <th>Column14</th>\n",
              "      <th>Column15</th>\n",
              "      <th>Column16</th>\n",
              "      <th>Column17</th>\n",
              "      <th>Column18</th>\n",
              "      <th>Column19</th>\n",
              "      <th>Column20</th>\n",
              "      <th>Column21</th>\n",
              "      <th>Column22</th>\n",
              "      <th>Column23</th>\n",
              "      <th>Column24</th>\n",
              "      <th>Column25</th>\n",
              "      <th>Column26</th>\n",
              "      <th>Column27</th>\n",
              "      <th>Column28</th>\n",
              "      <th>Column29</th>\n",
              "      <th>Column30</th>\n",
              "      <th>Column31</th>\n",
              "      <th>Column32</th>\n",
              "      <th>Column33</th>\n",
              "      <th>Column34</th>\n",
              "      <th>Column35</th>\n",
              "      <th>Column36</th>\n",
              "      <th>Column37</th>\n",
              "      <th>Column38</th>\n",
              "      <th>Column39</th>\n",
              "      <th>Column40</th>\n",
              "      <th>Column41</th>\n",
              "      <th>Column42</th>\n",
              "      <th>Column43</th>\n",
              "      <th>Column44</th>\n",
              "      <th>Column45</th>\n",
              "      <th>Column46</th>\n",
              "      <th>Column47</th>\n",
              "      <th>Column48</th>\n",
              "      <th>Column49</th>\n",
              "      <th>Column50</th>\n",
              "      <th>Column51</th>\n",
              "      <th>Column52</th>\n",
              "      <th>Column53</th>\n",
              "      <th>Column54</th>\n",
              "      <th>Column55</th>\n",
              "      <th>Column56</th>\n",
              "      <th>Column57</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>15</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Column0  Column1  Column2  Column3  ...  Column54  Column55  Column56  Column57\n",
              "0     0.21     0.28     0.50      0.0  ...     5.114       101      1028         1\n",
              "1     0.06     0.00     0.71      0.0  ...     9.821       485      2259         1\n",
              "2     0.00     0.00     0.00      0.0  ...     3.537        40       191         1\n",
              "3     0.00     0.00     0.00      0.0  ...     3.537        40       191         1\n",
              "4     0.00     0.00     0.00      0.0  ...     3.000        15        54         1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F23PWkGiC8g4",
        "colab_type": "code",
        "outputId": "57db31ba-0801-4fd6-e6ca-39ad6cb400c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "# Checking the last five observations\n",
        "df.tail()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column0</th>\n",
              "      <th>Column1</th>\n",
              "      <th>Column2</th>\n",
              "      <th>Column3</th>\n",
              "      <th>Column4</th>\n",
              "      <th>Column5</th>\n",
              "      <th>Column6</th>\n",
              "      <th>Column7</th>\n",
              "      <th>Column8</th>\n",
              "      <th>Column9</th>\n",
              "      <th>Column10</th>\n",
              "      <th>Column11</th>\n",
              "      <th>Column12</th>\n",
              "      <th>Column13</th>\n",
              "      <th>Column14</th>\n",
              "      <th>Column15</th>\n",
              "      <th>Column16</th>\n",
              "      <th>Column17</th>\n",
              "      <th>Column18</th>\n",
              "      <th>Column19</th>\n",
              "      <th>Column20</th>\n",
              "      <th>Column21</th>\n",
              "      <th>Column22</th>\n",
              "      <th>Column23</th>\n",
              "      <th>Column24</th>\n",
              "      <th>Column25</th>\n",
              "      <th>Column26</th>\n",
              "      <th>Column27</th>\n",
              "      <th>Column28</th>\n",
              "      <th>Column29</th>\n",
              "      <th>Column30</th>\n",
              "      <th>Column31</th>\n",
              "      <th>Column32</th>\n",
              "      <th>Column33</th>\n",
              "      <th>Column34</th>\n",
              "      <th>Column35</th>\n",
              "      <th>Column36</th>\n",
              "      <th>Column37</th>\n",
              "      <th>Column38</th>\n",
              "      <th>Column39</th>\n",
              "      <th>Column40</th>\n",
              "      <th>Column41</th>\n",
              "      <th>Column42</th>\n",
              "      <th>Column43</th>\n",
              "      <th>Column44</th>\n",
              "      <th>Column45</th>\n",
              "      <th>Column46</th>\n",
              "      <th>Column47</th>\n",
              "      <th>Column48</th>\n",
              "      <th>Column49</th>\n",
              "      <th>Column50</th>\n",
              "      <th>Column51</th>\n",
              "      <th>Column52</th>\n",
              "      <th>Column53</th>\n",
              "      <th>Column54</th>\n",
              "      <th>Column55</th>\n",
              "      <th>Column56</th>\n",
              "      <th>Column57</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4595</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.142</td>\n",
              "      <td>3</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4596</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.555</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4597</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.404</td>\n",
              "      <td>6</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4598</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.147</td>\n",
              "      <td>5</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4599</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.97</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.250</td>\n",
              "      <td>5</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Column0  Column1  Column2  ...  Column55  Column56  Column57\n",
              "4595     0.31      0.0     0.62  ...         3        88         0\n",
              "4596     0.00      0.0     0.00  ...         4        14         0\n",
              "4597     0.30      0.0     0.30  ...         6       118         0\n",
              "4598     0.96      0.0     0.00  ...         5        78         0\n",
              "4599     0.00      0.0     0.65  ...         5        40         0\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ONEPt_oC8d_",
        "colab_type": "code",
        "outputId": "e9bb4439-9946-4b16-d864-83e141a54186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "# Checking summary statistics from our dataset\n",
        "df.describe()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column0</th>\n",
              "      <th>Column1</th>\n",
              "      <th>Column2</th>\n",
              "      <th>Column3</th>\n",
              "      <th>Column4</th>\n",
              "      <th>Column5</th>\n",
              "      <th>Column6</th>\n",
              "      <th>Column7</th>\n",
              "      <th>Column8</th>\n",
              "      <th>Column9</th>\n",
              "      <th>Column10</th>\n",
              "      <th>Column11</th>\n",
              "      <th>Column12</th>\n",
              "      <th>Column13</th>\n",
              "      <th>Column14</th>\n",
              "      <th>Column15</th>\n",
              "      <th>Column16</th>\n",
              "      <th>Column17</th>\n",
              "      <th>Column18</th>\n",
              "      <th>Column19</th>\n",
              "      <th>Column20</th>\n",
              "      <th>Column21</th>\n",
              "      <th>Column22</th>\n",
              "      <th>Column23</th>\n",
              "      <th>Column24</th>\n",
              "      <th>Column25</th>\n",
              "      <th>Column26</th>\n",
              "      <th>Column27</th>\n",
              "      <th>Column28</th>\n",
              "      <th>Column29</th>\n",
              "      <th>Column30</th>\n",
              "      <th>Column31</th>\n",
              "      <th>Column32</th>\n",
              "      <th>Column33</th>\n",
              "      <th>Column34</th>\n",
              "      <th>Column35</th>\n",
              "      <th>Column36</th>\n",
              "      <th>Column37</th>\n",
              "      <th>Column38</th>\n",
              "      <th>Column39</th>\n",
              "      <th>Column40</th>\n",
              "      <th>Column41</th>\n",
              "      <th>Column42</th>\n",
              "      <th>Column43</th>\n",
              "      <th>Column44</th>\n",
              "      <th>Column45</th>\n",
              "      <th>Column46</th>\n",
              "      <th>Column47</th>\n",
              "      <th>Column48</th>\n",
              "      <th>Column49</th>\n",
              "      <th>Column50</th>\n",
              "      <th>Column51</th>\n",
              "      <th>Column52</th>\n",
              "      <th>Column53</th>\n",
              "      <th>Column54</th>\n",
              "      <th>Column55</th>\n",
              "      <th>Column56</th>\n",
              "      <th>Column57</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.104576</td>\n",
              "      <td>0.212922</td>\n",
              "      <td>0.280578</td>\n",
              "      <td>0.065439</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>0.095922</td>\n",
              "      <td>0.114233</td>\n",
              "      <td>0.105317</td>\n",
              "      <td>0.090087</td>\n",
              "      <td>0.239465</td>\n",
              "      <td>0.059837</td>\n",
              "      <td>0.541680</td>\n",
              "      <td>0.093950</td>\n",
              "      <td>0.058639</td>\n",
              "      <td>0.049215</td>\n",
              "      <td>0.248833</td>\n",
              "      <td>0.142617</td>\n",
              "      <td>0.184504</td>\n",
              "      <td>1.662041</td>\n",
              "      <td>0.085596</td>\n",
              "      <td>0.809728</td>\n",
              "      <td>0.121228</td>\n",
              "      <td>0.101667</td>\n",
              "      <td>0.094289</td>\n",
              "      <td>0.549624</td>\n",
              "      <td>0.265441</td>\n",
              "      <td>0.767472</td>\n",
              "      <td>0.124872</td>\n",
              "      <td>0.098937</td>\n",
              "      <td>0.102874</td>\n",
              "      <td>0.064767</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.097250</td>\n",
              "      <td>0.047846</td>\n",
              "      <td>0.105435</td>\n",
              "      <td>0.097498</td>\n",
              "      <td>0.136983</td>\n",
              "      <td>0.013204</td>\n",
              "      <td>0.078646</td>\n",
              "      <td>0.064848</td>\n",
              "      <td>0.043676</td>\n",
              "      <td>0.132367</td>\n",
              "      <td>0.046109</td>\n",
              "      <td>0.079213</td>\n",
              "      <td>0.301289</td>\n",
              "      <td>0.179863</td>\n",
              "      <td>0.005446</td>\n",
              "      <td>0.031876</td>\n",
              "      <td>0.038583</td>\n",
              "      <td>0.139061</td>\n",
              "      <td>0.016980</td>\n",
              "      <td>0.268960</td>\n",
              "      <td>0.075827</td>\n",
              "      <td>0.044248</td>\n",
              "      <td>5.191827</td>\n",
              "      <td>52.170870</td>\n",
              "      <td>283.290435</td>\n",
              "      <td>0.393913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.305387</td>\n",
              "      <td>1.290700</td>\n",
              "      <td>0.504170</td>\n",
              "      <td>1.395303</td>\n",
              "      <td>0.672586</td>\n",
              "      <td>0.273850</td>\n",
              "      <td>0.391480</td>\n",
              "      <td>0.401112</td>\n",
              "      <td>0.278643</td>\n",
              "      <td>0.644816</td>\n",
              "      <td>0.201565</td>\n",
              "      <td>0.861791</td>\n",
              "      <td>0.301065</td>\n",
              "      <td>0.335219</td>\n",
              "      <td>0.258871</td>\n",
              "      <td>0.825881</td>\n",
              "      <td>0.444099</td>\n",
              "      <td>0.530930</td>\n",
              "      <td>1.775669</td>\n",
              "      <td>0.509821</td>\n",
              "      <td>1.200938</td>\n",
              "      <td>1.025866</td>\n",
              "      <td>0.350321</td>\n",
              "      <td>0.442681</td>\n",
              "      <td>1.671511</td>\n",
              "      <td>0.887043</td>\n",
              "      <td>3.367639</td>\n",
              "      <td>0.538631</td>\n",
              "      <td>0.593389</td>\n",
              "      <td>0.456729</td>\n",
              "      <td>0.403435</td>\n",
              "      <td>0.328594</td>\n",
              "      <td>0.555966</td>\n",
              "      <td>0.329480</td>\n",
              "      <td>0.532315</td>\n",
              "      <td>0.402664</td>\n",
              "      <td>0.423493</td>\n",
              "      <td>0.220675</td>\n",
              "      <td>0.434718</td>\n",
              "      <td>0.349953</td>\n",
              "      <td>0.361243</td>\n",
              "      <td>0.766900</td>\n",
              "      <td>0.223835</td>\n",
              "      <td>0.622042</td>\n",
              "      <td>1.011787</td>\n",
              "      <td>0.911214</td>\n",
              "      <td>0.076283</td>\n",
              "      <td>0.285765</td>\n",
              "      <td>0.243497</td>\n",
              "      <td>0.270377</td>\n",
              "      <td>0.109406</td>\n",
              "      <td>0.815726</td>\n",
              "      <td>0.245906</td>\n",
              "      <td>0.429388</td>\n",
              "      <td>31.732891</td>\n",
              "      <td>194.912453</td>\n",
              "      <td>606.413764</td>\n",
              "      <td>0.488669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.588000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.310000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.275500</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.382500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.640000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.314250</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.705250</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>265.250000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.540000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>42.810000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>7.270000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>5.260000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>2.610000</td>\n",
              "      <td>9.670000</td>\n",
              "      <td>5.550000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.410000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>17.100000</td>\n",
              "      <td>5.450000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>20.830000</td>\n",
              "      <td>16.660000</td>\n",
              "      <td>33.330000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.690000</td>\n",
              "      <td>6.890000</td>\n",
              "      <td>8.330000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>3.570000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.420000</td>\n",
              "      <td>22.050000</td>\n",
              "      <td>2.170000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.385000</td>\n",
              "      <td>9.752000</td>\n",
              "      <td>4.081000</td>\n",
              "      <td>32.478000</td>\n",
              "      <td>6.003000</td>\n",
              "      <td>19.829000</td>\n",
              "      <td>1102.500000</td>\n",
              "      <td>9989.000000</td>\n",
              "      <td>15841.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Column0      Column1  ...      Column56     Column57\n",
              "count  4600.000000  4600.000000  ...   4600.000000  4600.000000\n",
              "mean      0.104576     0.212922  ...    283.290435     0.393913\n",
              "std       0.305387     1.290700  ...    606.413764     0.488669\n",
              "min       0.000000     0.000000  ...      1.000000     0.000000\n",
              "25%       0.000000     0.000000  ...     35.000000     0.000000\n",
              "50%       0.000000     0.000000  ...     95.000000     0.000000\n",
              "75%       0.000000     0.000000  ...    265.250000     1.000000\n",
              "max       4.540000    14.280000  ...  15841.000000     1.000000\n",
              "\n",
              "[8 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNWTf4zxMiTY",
        "colab_type": "text"
      },
      "source": [
        "The dataset has too many columns to analyse manually.\n",
        "we really need reduction methods here to find the most important features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7HXd7CAC8bJ",
        "colab_type": "code",
        "outputId": "a0b4675c-52cd-458a-ed93-86233a7257b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# checking summary information about our dataset\n",
        "df.info()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4600 entries, 0 to 4599\n",
            "Data columns (total 58 columns):\n",
            "Column0     4600 non-null float64\n",
            "Column1     4600 non-null float64\n",
            "Column2     4600 non-null float64\n",
            "Column3     4600 non-null float64\n",
            "Column4     4600 non-null float64\n",
            "Column5     4600 non-null float64\n",
            "Column6     4600 non-null float64\n",
            "Column7     4600 non-null float64\n",
            "Column8     4600 non-null float64\n",
            "Column9     4600 non-null float64\n",
            "Column10    4600 non-null float64\n",
            "Column11    4600 non-null float64\n",
            "Column12    4600 non-null float64\n",
            "Column13    4600 non-null float64\n",
            "Column14    4600 non-null float64\n",
            "Column15    4600 non-null float64\n",
            "Column16    4600 non-null float64\n",
            "Column17    4600 non-null float64\n",
            "Column18    4600 non-null float64\n",
            "Column19    4600 non-null float64\n",
            "Column20    4600 non-null float64\n",
            "Column21    4600 non-null float64\n",
            "Column22    4600 non-null float64\n",
            "Column23    4600 non-null float64\n",
            "Column24    4600 non-null float64\n",
            "Column25    4600 non-null float64\n",
            "Column26    4600 non-null float64\n",
            "Column27    4600 non-null float64\n",
            "Column28    4600 non-null float64\n",
            "Column29    4600 non-null float64\n",
            "Column30    4600 non-null float64\n",
            "Column31    4600 non-null float64\n",
            "Column32    4600 non-null float64\n",
            "Column33    4600 non-null float64\n",
            "Column34    4600 non-null float64\n",
            "Column35    4600 non-null float64\n",
            "Column36    4600 non-null float64\n",
            "Column37    4600 non-null float64\n",
            "Column38    4600 non-null float64\n",
            "Column39    4600 non-null float64\n",
            "Column40    4600 non-null float64\n",
            "Column41    4600 non-null float64\n",
            "Column42    4600 non-null float64\n",
            "Column43    4600 non-null float64\n",
            "Column44    4600 non-null float64\n",
            "Column45    4600 non-null float64\n",
            "Column46    4600 non-null float64\n",
            "Column47    4600 non-null float64\n",
            "Column48    4600 non-null float64\n",
            "Column49    4600 non-null float64\n",
            "Column50    4600 non-null float64\n",
            "Column51    4600 non-null float64\n",
            "Column52    4600 non-null float64\n",
            "Column53    4600 non-null float64\n",
            "Column54    4600 non-null float64\n",
            "Column55    4600 non-null int64\n",
            "Column56    4600 non-null int64\n",
            "Column57    4600 non-null int64\n",
            "dtypes: float64(55), int64(3)\n",
            "memory usage: 2.0 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vWbMbhHMClr",
        "colab_type": "text"
      },
      "source": [
        "* The data has 4600 rows and 58 columns\n",
        "* We have no missing values\n",
        "* The columns are too many and we need to used reduction methods to reduce the data dimension.\n",
        "* We will go straight to modelling since even making charts can't help in analysing the columns.\n",
        "* We will plot the target variable though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVZyDIT2SH5A",
        "colab_type": "code",
        "outputId": "130eb00a-a295-4e2c-98fd-76bb609aab58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# Plotting the target variable \n",
        "# using seaborn\n",
        "\n",
        "sns.countplot(df.Column57)\n",
        "plt.title('Spam vs Non_Spam Emails')\n",
        "plt.show()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEcCAYAAAAV2MmlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGMIGQCAMf8PYSJQ5mMHfYQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUWYLu71rACY",
        "colab_type": "text"
      },
      "source": [
        "* The target variable has two classes, the spam, and non spam emails.\n",
        "\n",
        "* From the chart we have more non spam emails compared to spam emails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChN8Lks7NS_W",
        "colab_type": "text"
      },
      "source": [
        "# 2.2 Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn-wL-exNWBu",
        "colab_type": "text"
      },
      "source": [
        "## 2.2.1 Naive Bayes Classifier\n",
        "\n",
        "* The Naive Bayes Classifier is a statistical classification technique based on the Bayes Theorem. \n",
        "\n",
        "* It has high accuracy and speed on large datasets.\n",
        "\n",
        "* This type of classifier takes into account the assumption that the effect of a particular feature in a class is independent of other features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQsOCFXxQF7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create correlation matrix\n",
        "corr_matrix = df.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find index of feature columns with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "\n",
        "# Drop the highly correlated features \n",
        "df.drop(df[to_drop], axis=1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mBAgw1xRA0x",
        "colab_type": "code",
        "outputId": "d686d8d3-683d-44a2-d0dc-abefd7268eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# checking if there are any correlated features\n",
        "to_drop"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Column33']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBB4Vcp_RNq4",
        "colab_type": "code",
        "outputId": "268f3b8d-8617-4172-fd2d-7444b2ef9d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# checking if there are any columns dropped\n",
        "df.shape"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4600, 57)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl1yinY2RWa4",
        "colab_type": "text"
      },
      "source": [
        "No columns were dropped since there are no correlated features.\n",
        "we will use PCA and LDA to reduce the data dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo0OZWzIt4LD",
        "colab_type": "text"
      },
      "source": [
        "Splitting the Spam data into 80, 20 train and test sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avvIHbFSC8R-",
        "colab_type": "code",
        "outputId": "8a72856f-b784-4d1a-f83d-fce8381edf5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "# Fitting the Naives Bayes Classifier: GausssianNB since the features are continuous\n",
        "# Splitting the data\n",
        "\n",
        "X = df.iloc[:, 0:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "# transform = Normalizer()\n",
        "# X = transform.transform(X)\n",
        "\n",
        "# X = normalize(X, norm = 'l2')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "sc = StandardScaler(with_std = False, with_mean = False)\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# pca = PCA(n_components = 1)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "model = gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9032608695652173\n",
            "\n",
            "\n",
            "[[513  25]\n",
            " [ 64 318]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.95      0.92       538\n",
            "           1       0.93      0.83      0.88       382\n",
            "\n",
            "    accuracy                           0.90       920\n",
            "   macro avg       0.91      0.89      0.90       920\n",
            "weighted avg       0.90      0.90      0.90       920\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wEVxWw1tyAh",
        "colab_type": "text"
      },
      "source": [
        "* The 80,20 split model yielded 90.3% accuracy.\n",
        "* Interpreting the confusion matrix;\n",
        "* The first row is about the non-spam-predictions:\n",
        "     * 513 emails were correctly classified as Ham (called true negatives) \n",
        "     * 25 were wrongly classified as ham (false positives).\n",
        "* The second row is about the spam-predictions: \n",
        "     * 64 emails where wrongly classified as spam (false negatives) and\n",
        "     * 318 were correctly classified as Spam (true positives).\n",
        "     \n",
        "* In this classification we value the Recall so much. \n",
        "* The Recall is 95% which means that this is good model.\n",
        "* Recall is the ability of a model to find all the relevant cases within a dataset.\n",
        "* It is also called sensitivity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iLUn8qrC8Ly",
        "colab_type": "code",
        "outputId": "1ae55910-94eb-4723-859d-6c4b26d862a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "# Fitting the Naives Bayes Classifier: GausssianNB since the features are continuous\n",
        "# Splitting the data\n",
        "\n",
        "X = df.iloc[:, 0:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "# transform = Normalizer()\n",
        "# X = transform.transform(X)\n",
        "\n",
        "# X = normalize(X, norm = 'l2')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "sc = StandardScaler(with_std = False, with_mean = False)\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# pca = PCA(n_components = 1)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "model = gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9036231884057971\n",
            "\n",
            "\n",
            "[[789  33]\n",
            " [100 458]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92       822\n",
            "           1       0.93      0.82      0.87       558\n",
            "\n",
            "    accuracy                           0.90      1380\n",
            "   macro avg       0.91      0.89      0.90      1380\n",
            "weighted avg       0.91      0.90      0.90      1380\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE3KTnWRvroa",
        "colab_type": "text"
      },
      "source": [
        "* The 70,30 split model also yielded 90.3% accuracy.\n",
        "* Interpreting the confusion matrix;\n",
        "* The first row is about the non-spam-predictions:\n",
        "     * 789 emails were correctly classified as Ham (called true negatives) \n",
        "     * 33 were wrongly classified as ham (false positives).\n",
        "* The second row is about the spam-predictions: \n",
        "     * 100 emails where wrongly classified as spam (false negatives) and\n",
        "     * 458 were correctly classified as Spam (true positives).\n",
        "     \n",
        "* The recall improved slightly from 95% in the previous model to 96%. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKNIHGDwC8Ii",
        "colab_type": "code",
        "outputId": "43c5118a-122c-4c31-e0d0-f4eef801e8f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "# Fitting the Naives Bayes Classifier: GausssianNB since the features are continuous\n",
        "# Splitting the data\n",
        "\n",
        "X = df.iloc[:, 0:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "# transform = Normalizer()\n",
        "# X = transform.transform(X)\n",
        "\n",
        "# X = normalize(X, norm = 'l2')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 0)\n",
        "\n",
        "sc = StandardScaler(with_std = False, with_mean = False)\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# pca = PCA(n_components = 1)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "model = gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9081521739130435\n",
            "\n",
            "\n",
            "[[1060   37]\n",
            " [ 132  611]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.97      0.93      1097\n",
            "           1       0.94      0.82      0.88       743\n",
            "\n",
            "    accuracy                           0.91      1840\n",
            "   macro avg       0.92      0.89      0.90      1840\n",
            "weighted avg       0.91      0.91      0.91      1840\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnzYoXY6v_pg",
        "colab_type": "text"
      },
      "source": [
        "* The 60,40 split model yielded 90.8% accuracy\n",
        "* This is a slight improvement compared to the previous split models.\n",
        "* Interpreting the confusion matrix;\n",
        "* The first row is about the non-spam-predictions:\n",
        "     * 1060 emails were correctly classified as Ham (called true negatives) \n",
        "     * 37 were wrongly classified as ham (false positives).\n",
        "* The second row is about the spam-predictions: \n",
        "     * 132 emails where wrongly classified as spam (false negatives) and\n",
        "     * 611 were correctly classified as Spam (true positives)..\n",
        "     \n",
        "* Again here the recall increased to 97%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v60TTtzPC8Fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgEIVSoSU1sI",
        "colab_type": "text"
      },
      "source": [
        "## 2.2.2 Optimizing Naive Bayes\n",
        "Improving the performance of the Naive Bayes classifier:\n",
        " * Normalizing our data\n",
        " * Remove Redundant/ correlated features\n",
        " * Apply smoothing techniques; If our dataset has zero frequency issue, we can apply smoothing techniques such as \"Laplace Correction\" to predict the class of the test data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg2XweeWVmW1",
        "colab_type": "text"
      },
      "source": [
        "We have already applied the first techniques in our models which was relevant in this scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89tAyiKiVYM7",
        "colab_type": "text"
      },
      "source": [
        "## 2.2.3 Recommendation\n",
        "\n",
        "* Normalizing or standardizing the features works greatly to improve the classifier.\n",
        "* For the spam detection challenge project, using the Standard Scaler yielded the best results.\n",
        "* Also, using the linear discriminant analysis for dimension reduction yields beter results compared to PCA.\n",
        "* Lastly increasing the test size in the spam dataset imporved both the accuracy and the Recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLNahUCk1QDT",
        "colab_type": "text"
      },
      "source": [
        "## 2.2.4 Challenging the Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wAf00gA1ZSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "d030e7a9-e1c2-4349-a63d-168e71919a43"
      },
      "source": [
        "# Fitting the Support Vector Classifier\n",
        "# Splitting the data\n",
        "\n",
        "X = df.iloc[:, 0:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "# transform = Normalizer()\n",
        "# X = transform.transform(X)\n",
        "\n",
        "# X = normalize(X, norm = 'l2')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 0)\n",
        "\n",
        "sc = StandardScaler(with_std = False, with_mean = False)\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# pca = PCA(n_components = 1)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "svc = SVC(C=0.1, gamma=0.001, kernel = 'linear')\n",
        "\n",
        "model = svc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9228260869565217\n",
            "\n",
            "\n",
            "[[1043   54]\n",
            " [  88  655]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.94      1097\n",
            "           1       0.92      0.88      0.90       743\n",
            "\n",
            "    accuracy                           0.92      1840\n",
            "   macro avg       0.92      0.92      0.92      1840\n",
            "weighted avg       0.92      0.92      0.92      1840\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SoVhOcb3_5b",
        "colab_type": "text"
      },
      "source": [
        "* The Support Vector Machine model yielded 92.2% accuracy.\n",
        "* This is a great improvement compared to the Naive Bayes Gaussian models.\n",
        "* Interpreting the confusion matrix;\n",
        "* The first row is about the non-spam-predictions:\n",
        "     * 10 emails were correctly classified as Ham (called true negatives) \n",
        "     * 54 were wrongly classified as ham (false positives).\n",
        "* The second row is about the spam-predictions: \n",
        "     * 88 emails where wrongly classified as spam (false negatives) and\n",
        "     * 655 were correctly classified as Spam (true positives)..\n",
        "     \n",
        "* Again here the recall increased to 95%.\n",
        "* This is slight reduction.\n",
        "* Though the accuracy improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31ibSV8L2Q2v",
        "colab_type": "text"
      },
      "source": [
        "## 2.2.5 Conclusion\n",
        "* The Gaussian NB is the best model since it yielded both the best Accuracy and Recall scores.\n",
        "* Optimizing the model is very essential:\n",
        "    1. Scaling or normalizing the features\n",
        "    2. Reducing the data dimensions\n",
        "    3. Increasing the test size for a large dataset."
      ]
    }
  ]
}